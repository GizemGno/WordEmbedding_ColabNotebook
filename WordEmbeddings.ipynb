{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordEmbeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GizemGno/WordEmbedding_ColabNotebook/blob/master/WordEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmF270rXIKW9",
        "colab_type": "text"
      },
      "source": [
        "Notebook'un orijinali: https://www.tensorflow.org/tutorials/text/word_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWZPGMFSJA3c",
        "colab_type": "text"
      },
      "source": [
        "Makine Öğrenmesi ile metin işlerken girdiyi modele sayı vektörleri şeklinde veriyoruz. Bunun için birkaç yol var, referans aldığımız tensorflow notebook'u üç stratejiyi ele almış.\n",
        "\n",
        "\n",
        "**One-Hot Encoding**\n",
        "\n",
        "Bu yöntemde her bir benzersiz kelimeyi one-hot vektöler ile ifade ediyoruz. Burada vektör boyutu sözlük boyutu kadar ve her birinde ayrı bir indeks dışındaki tüm indeksler sıfır oluyor. \n",
        "\n",
        "Mesela örnek cümlemiz \"İki kere iki dört eder\" olsun. Burada sözlüğümüzdeki kelimeler (iki, kere, dört, eder), vektör boyutumuz 4, örnek gösterim:\n",
        "\n",
        "iki = [1 0 0 0]\n",
        "\n",
        "kere = [0 1 0 0]\n",
        "\n",
        "dört = [0 0 1 0]\n",
        "\n",
        "eder = [0 0 0 1]\n",
        "\n",
        "\n",
        "\n",
        "Cümleyi kodlamak için içerdiği kelimelerin one-hot vektörlerini birleştirmemiz yeterli olacak. \n",
        "\n",
        "Vektörlerdeki elementlerin çoğu 0 oluduğu için (bu vektörlere sparse vektör deniyor) bu yöntem verimsiz bir yöntem. \n",
        "\n",
        "**Her Kelimeyi Bir Sayı ile Gösterme**\n",
        "\n",
        "Bu yöntemde one-hot encode vektörler yerine her kelimeyi bir sayı ile gösteriyoruz. Mesela önceki örnekteki cümleyi (1,2,1,3,4) şeklinde ifade edebiliriz. Artık sparse değil dense vektör elde ettiğimizden yöntem daha verimli. \n",
        "\n",
        "Bu yöntemin dezavantaji ise kelimelerin arasındaki ilişkiyi yakalayamıyor olması. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sxceG2vbtb-",
        "colab_type": "text"
      },
      "source": [
        "**Word Embeddings**\n",
        "\n",
        "Word embeddings ile ifade edilen kelimeler hem dense oluyor, hem de kelimelerin arasındaki ilişki yakalanmış oluyor. Burada vektör uzunluğu belirlediğimiz parametre sayısı kadar ve her indis yüzdelik bir sayı oluyor. Yöntemin en iyi yanlarından biri bu değerleri el ile vermememiz, bunun yerine bu değerler modelin eğitilmesi sırasında öğreniliyor. Küçük veri setlerinde 8 boyutlu embeddingler yaygın görülürken, daha büyük veri setlerinde bu sayı 1024'e kadar çıkabiliyor. \n",
        "\n",
        "*4 Boyutlu Embedding Örneği*\n",
        "\n",
        "iki => [1.2 -0.1 4.3 3.2]\n",
        "kere => [0.4 2.5 -0.9 0.5]\n",
        "dört => [2.1 0.3 0.1 0.4]\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0ZZRpoeHvJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Setup\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  # Tensorflow 2.0 versiyonunu kullanmak için\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "#tensorflow dataset\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3JtXRRdhX0-",
        "colab_type": "text"
      },
      "source": [
        "**Keras Embedding Katmanı**\n",
        "\n",
        "Embedding katmanı belirli bir kelimeye karşılık gelen tamsayıları dense vektörlere eşleyen bir lookup tablosu olarak düşünülebilir. Embedding'in boyutu bir hyperparameter, uygulanacak probleme en uygun değer test edilerek bulunmalı. \n",
        "\n",
        "Embedding katman oluşturulduğunda, katmanın ağırlıklarına rasgale değerler atanıyor. Modelin eğitilmesi sırasında backpropagation ile bu ağırlıklar güncelleniyor. Eğitildikten sonra, öğrenilen kelime vektörleri kelimeler arasındaki benzerlikleri kabaca kodlamış oluyor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drT01WSFhGsf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "879dc36e-8694-4c1a-c039-2932aa355960"
      },
      "source": [
        "# Tensorflow Dataset'ten imdb yorumlarini yukleyelim\n",
        "\n",
        "(train_data, test_data), info = tfds.load(\n",
        "    'imdb_reviews/subwords8k', \n",
        "    split = (tfds.Split.TRAIN, tfds.Split.TEST), \n",
        "    with_info=True, as_supervised=True)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset imdb_reviews (80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/subwords8k/0.1.0...\u001b[0m\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_datasets/core/file_format_adapter.py:209: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_datasets/core/file_format_adapter.py:209: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/subwords8k/0.1.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImbY-8h8oT9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Film yorumları herhangi uzunlukta olabilir. \n",
        "# padded_batch metodunu kullanarak uzunlukları standart hale getirecegiz\n",
        "\n",
        "padded_shapes = ([None],())\n",
        "train_batches = train_data.shuffle(1000).padded_batch(10, padded_shapes = padded_shapes)\n",
        "test_batches = test_data.shuffle(1000).padded_batch(10, padded_shapes = padded_shapes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcM6PI9Wrx91",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "70e747c1-9ed0-4b89-a296-a7612a87d248"
      },
      "source": [
        "embedding_dim=16\n",
        "encoder = info.features['text'].encoder\n",
        "\n",
        "model = keras.Sequential([\n",
        "  layers.Embedding(encoder.vocab_size, embedding_dim),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Modeli compile edip eğitelim\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss = 'binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_batches,\n",
        "    epochs = 10,\n",
        "    validation_data = test_batches, validation_steps=20)\n",
        "\n",
        "# Burada accuracy ve loss kısımlarına dikkat ederek modelin performansına \n",
        "# bakabiliriz"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 16)          130960    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 130,977\n",
            "Trainable params: 130,977\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "2500/2500 [==============================] - 23s 9ms/step - loss: 0.6360 - accuracy: 0.6926 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "2500/2500 [==============================] - 16s 6ms/step - loss: 0.4635 - accuracy: 0.8361 - val_loss: 0.3938 - val_accuracy: 0.8700\n",
            "Epoch 3/10\n",
            "2500/2500 [==============================] - 16s 6ms/step - loss: 0.3616 - accuracy: 0.8780 - val_loss: 0.3462 - val_accuracy: 0.8550\n",
            "Epoch 4/10\n",
            "2500/2500 [==============================] - 16s 6ms/step - loss: 0.3074 - accuracy: 0.8959 - val_loss: 0.3289 - val_accuracy: 0.8850\n",
            "Epoch 5/10\n",
            "2500/2500 [==============================] - 16s 6ms/step - loss: 0.2724 - accuracy: 0.9070 - val_loss: 0.3414 - val_accuracy: 0.8750\n",
            "Epoch 6/10\n",
            "2500/2500 [==============================] - 16s 6ms/step - loss: 0.2496 - accuracy: 0.9149 - val_loss: 0.2620 - val_accuracy: 0.9150\n",
            "Epoch 7/10\n",
            "2500/2500 [==============================] - 16s 6ms/step - loss: 0.2312 - accuracy: 0.9239 - val_loss: 0.3478 - val_accuracy: 0.8550\n",
            "Epoch 8/10\n",
            "2500/2500 [==============================] - 16s 7ms/step - loss: 0.2147 - accuracy: 0.9307 - val_loss: 0.2850 - val_accuracy: 0.9000\n",
            "Epoch 9/10\n",
            "2500/2500 [==============================] - 16s 6ms/step - loss: 0.1998 - accuracy: 0.9352 - val_loss: 0.2191 - val_accuracy: 0.8900\n",
            "Epoch 10/10\n",
            "2500/2500 [==============================] - 16s 6ms/step - loss: 0.1913 - accuracy: 0.9381 - val_loss: 0.3419 - val_accuracy: 0.8650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA1pNDKNs5X8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ogrenilen word embeddingler (vocab_size, embedding_dim) boyutunda bir matris \n",
        "# Asagidaki kod ile bu embeddingleri bir degiskene atabiliriz\n",
        "\n",
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "\n",
        "# Asagidaki kod ile agirliklari diske yazabiliriz\n",
        "\n",
        "import io\n",
        "\n",
        "\n",
        "out_v = io.open('vecs.tsv','w',encoding='utf-8')\n",
        "out_m = io.open('meta.tsv','w',encoding='utf-8')\n",
        "\n",
        "for num, word in enumerate(encoder.subwords):\n",
        "  vec = weights[num+1] # 0 numara padding oldugundan atliyoruz\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnAfM2-_v4Lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Colab'da calisiyorsaniz asagidaki kod ile az once olusturdugumuz\n",
        "# dosyalari local bilgisayariniza indirip tensorflow projector ile \n",
        "# modelinizi gorsel olarak inceleyebilirsiniz\n",
        "\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "   pass\n",
        "else:\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
